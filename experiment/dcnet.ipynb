{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 62 µs, sys: 7 µs, total: 69 µs\n",
      "Wall time: 76.1 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import os \n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./experiment/log/cityscapes/resnet_fcn_asp_deepbase_resnet101_dilated8_2023-05-04_22-04-07.log\n",
      "World size: 4\n",
      "['--configs', 'configs/cityscapes/R_101_D_8.json', '--drop_last', 'y', '--phase', 'train', '--gathered', 'n', '--loss_balance', 'y', '--log_to_file', 'n', '--backbone', 'deepbase_resnet101_dilated8', '--model_name', 'resnet_fcn_asp', '--gpu', '3', '4', '5', '6', '--train_batch_size', '8', '--val_batch_size', '4', '--data_dir', '../../input/openseg-cityscapes-gtfine', '--loss_type', 'fs_auxce_loss', '--max_iters', '40000', '--checkpoints_name', 'resnet_fcn_asp_deepbase_resnet101_dilated8_2023-05-04_22-04-07', '--pretrained', '../../input/pre-trained/resnet101-imagenet-openseg.pth', '--distributed']\n",
      "/home/zk/anaconda3/envs/temp/lib/python3.7/site-packages/torch/distributed/launch.py:186: FutureWarning: The module torch.distributed.launch is deprecated\n",
      "and will be removed in future. Use torchrun.\n",
      "Note that --use_env is set by default in torchrun.\n",
      "If your script expects `--local_rank` argument to be set, please\n",
      "change it to read from `os.environ['LOCAL_RANK']` instead. See \n",
      "https://pytorch.org/docs/stable/distributed.html#launch-utility for \n",
      "further instructions\n",
      "\n",
      "  FutureWarning,\n",
      "WARNING:torch.distributed.run:\n",
      "*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "2023-05-04 22:04:13,129 INFO    [offset_helper.py, 54] engery/max-distance: 5 engery/min-distance: 0\n",
      "2023-05-04 22:04:13,129 INFO    [offset_helper.py, 61] direction/num_classes: 8 scale: 1\n",
      "2023-05-04 22:04:13,129 INFO    [offset_helper.py, 66] c4 align axis: False\n",
      "2023-05-04 22:04:13,138 INFO    [offset_helper.py, 54] engery/max-distance: 5 engery/min-distance: 0\n",
      "2023-05-04 22:04:13,138 INFO    [offset_helper.py, 61] direction/num_classes: 8 scale: 1\n",
      "2023-05-04 22:04:13,138 INFO    [offset_helper.py, 66] c4 align axis: False\n",
      "2023-05-04 22:04:13,138 INFO    [offset_helper.py, 54] engery/max-distance: 5 engery/min-distance: 0\n",
      "2023-05-04 22:04:13,138 INFO    [offset_helper.py, 61] direction/num_classes: 8 scale: 1\n",
      "2023-05-04 22:04:13,138 INFO    [offset_helper.py, 66] c4 align axis: False\n",
      "2023-05-04 22:04:13,140 INFO    [offset_helper.py, 54] engery/max-distance: 5 engery/min-distance: 0\n",
      "2023-05-04 22:04:13,140 INFO    [offset_helper.py, 61] direction/num_classes: 8 scale: 1\n",
      "2023-05-04 22:04:13,140 INFO    [offset_helper.py, 66] c4 align axis: False\n",
      "2023-05-04 22:04:13,145 INFO    [module_runner.py, 44] BN Type is torchsyncbn.\n",
      "2023-05-04 22:04:13,145 INFO    [__init__.py, 17] Using evaluator: StandardEvaluator\n",
      "2023-05-04 22:04:13,153 INFO    [module_runner.py, 44] BN Type is torchsyncbn.\n",
      "2023-05-04 22:04:13,153 INFO    [__init__.py, 17] Using evaluator: StandardEvaluator\n",
      "2023-05-04 22:04:13,155 INFO    [module_runner.py, 44] BN Type is torchsyncbn.\n",
      "2023-05-04 22:04:13,155 INFO    [__init__.py, 17] Using evaluator: StandardEvaluator\n",
      "2023-05-04 22:04:13,155 INFO    [module_runner.py, 44] BN Type is torchsyncbn.\n",
      "2023-05-04 22:04:13,156 INFO    [__init__.py, 17] Using evaluator: StandardEvaluator\n",
      "2023-05-04 22:04:14,034 INFO    [module_helper.py, 128] Loading pretrained model:../../input/pre-trained/resnet101-imagenet-openseg.pth\n",
      "2023-05-04 22:04:14,042 INFO    [module_helper.py, 128] Loading pretrained model:../../input/pre-trained/resnet101-imagenet-openseg.pth\n",
      "2023-05-04 22:04:14,047 INFO    [module_helper.py, 128] Loading pretrained model:../../input/pre-trained/resnet101-imagenet-openseg.pth\n",
      "2023-05-04 22:04:14,060 INFO    [module_helper.py, 128] Loading pretrained model:../../input/pre-trained/resnet101-imagenet-openseg.pth\n",
      "2023-05-04 22:04:25,933 INFO    [trainer.py, 80] Params Group Method: None\n",
      "2023-05-04 22:04:25,935 INFO    [trainer.py, 80] Params Group Method: None\n",
      "2023-05-04 22:04:25,937 INFO    [optim_scheduler.py, 90] Use lambda_poly policy with default power 0.9\n",
      "2023-05-04 22:04:25,938 INFO    [data_loader.py, 131] use the DefaultLoader for train...\n",
      "2023-05-04 22:04:25,939 INFO    [optim_scheduler.py, 90] Use lambda_poly policy with default power 0.9\n",
      "2023-05-04 22:04:25,940 INFO    [data_loader.py, 131] use the DefaultLoader for train...\n",
      "DistributedDataParallel(\n",
      "  (module): RES_FCN_ASP(\n",
      "    (backbone): DilatedResnetBackbone(\n",
      "      (resinit): Sequential(\n",
      "        (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu1): ReLU()\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu2): ReLU()\n",
      "        (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn3): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu3): ReLU()\n",
      "      )\n",
      "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=True)\n",
      "      (layer1): Sequential(\n",
      "        (0): Bottleneck(\n",
      "          (conv1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU()\n",
      "          (relu_in): ReLU(inplace=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): Bottleneck(\n",
      "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU()\n",
      "          (relu_in): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Bottleneck(\n",
      "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): SyncBatchNorm(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU()\n",
      "          (relu_in): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (layer2): Sequential(\n",
      "        (0): Bottleneck(\n",
      "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn2): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU()\n",
      "          (relu_in): ReLU(inplace=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): Bottleneck(\n",
      "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU()\n",
      "          (relu_in): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Bottleneck(\n",
      "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU()\n",
      "          (relu_in): ReLU(inplace=True)\n",
      "        )\n",
      "        (3): Bottleneck(\n",
      "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): SyncBatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU()\n",
      "          (relu_in): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (layer3): Sequential(\n",
      "        (0): Bottleneck(\n",
      "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU()\n",
      "          (relu_in): ReLU(inplace=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "          (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU()\n",
      "          (relu_in): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "          (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU()\n",
      "          (relu_in): ReLU(inplace=True)\n",
      "        )\n",
      "        (3): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "          (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU()\n",
      "          (relu_in): ReLU(inplace=True)\n",
      "        )\n",
      "        (4): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "          (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU()\n",
      "          (relu_in): ReLU(inplace=True)\n",
      "        )\n",
      "        (5): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "          (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU()\n",
      "          (relu_in): ReLU(inplace=True)\n",
      "        )\n",
      "        (6): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "          (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU()\n",
      "          (relu_in): ReLU(inplace=True)\n",
      "        )\n",
      "        (7): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "          (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU()\n",
      "          (relu_in): ReLU(inplace=True)\n",
      "        )\n",
      "        (8): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "          (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU()\n",
      "          (relu_in): ReLU(inplace=True)\n",
      "        )\n",
      "        (9): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "          (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU()\n",
      "          (relu_in): ReLU(inplace=True)\n",
      "        )\n",
      "        (10): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "          (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU()\n",
      "          (relu_in): ReLU(inplace=True)\n",
      "        )\n",
      "        (11): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "          (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU()\n",
      "          (relu_in): ReLU(inplace=True)\n",
      "        )\n",
      "        (12): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "          (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU()\n",
      "          (relu_in): ReLU(inplace=True)\n",
      "        )\n",
      "        (13): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "          (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU()\n",
      "          (relu_in): ReLU(inplace=True)\n",
      "        )\n",
      "        (14): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "          (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU()\n",
      "          (relu_in): ReLU(inplace=True)\n",
      "        )\n",
      "        (15): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "          (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU()\n",
      "          (relu_in): ReLU(inplace=True)\n",
      "        )\n",
      "        (16): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "          (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU()\n",
      "          (relu_in): ReLU(inplace=True)\n",
      "        )\n",
      "        (17): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "          (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU()\n",
      "          (relu_in): ReLU(inplace=True)\n",
      "        )\n",
      "        (18): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "          (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU()\n",
      "          (relu_in): ReLU(inplace=True)\n",
      "        )\n",
      "        (19): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "          (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU()\n",
      "          (relu_in): ReLU(inplace=True)\n",
      "        )\n",
      "        (20): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "          (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU()\n",
      "          (relu_in): ReLU(inplace=True)\n",
      "        )\n",
      "        (21): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "          (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU()\n",
      "          (relu_in): ReLU(inplace=True)\n",
      "        )\n",
      "        (22): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "          (bn2): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): SyncBatchNorm(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU()\n",
      "          (relu_in): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (layer4): Sequential(\n",
      "        (0): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "          (bn2): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU()\n",
      "          (relu_in): ReLU(inplace=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): Bottleneck(\n",
      "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
      "          (bn2): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU()\n",
      "          (relu_in): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Bottleneck(\n",
      "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
      "          (bn2): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): SyncBatchNorm(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU()\n",
      "          (relu_in): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (fcn_asp_head): FCN_ASP(\n",
      "      (fcn): Sequential(\n",
      "        (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): Sequential(\n",
      "          (0): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (1): ReLU()\n",
      "        )\n",
      "      )\n",
      "      (conv3): Sequential(\n",
      "        (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(1, 1), padding=(12, 12), dilation=(12, 12))\n",
      "        (1): Sequential(\n",
      "          (0): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (1): ReLU()\n",
      "        )\n",
      "      )\n",
      "      (conv4): Sequential(\n",
      "        (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(1, 1), padding=(24, 24), dilation=(24, 24))\n",
      "        (1): Sequential(\n",
      "          (0): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (1): ReLU()\n",
      "        )\n",
      "      )\n",
      "      (conv5): Sequential(\n",
      "        (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(1, 1), padding=(36, 36), dilation=(36, 36))\n",
      "        (1): Sequential(\n",
      "          (0): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (1): ReLU()\n",
      "        )\n",
      "      )\n",
      "      (conv_bn_dropout): Sequential(\n",
      "        (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): Sequential(\n",
      "          (0): SyncBatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (1): ReLU()\n",
      "        )\n",
      "        (2): Dropout2d(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (head): Conv2d(256, 19, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (dsn_head): Sequential(\n",
      "      (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): Sequential(\n",
      "        (0): SyncBatchNorm(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (1): ReLU()\n",
      "      )\n",
      "      (2): Dropout2d(p=0.1, inplace=False)\n",
      "      (3): Conv2d(512, 19, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      ")\n",
      "2023-05-04 22:04:25,940 INFO    [trainer.py, 80] Params Group Method: None\n",
      "2023-05-04 22:04:25,940 INFO    [trainer.py, 80] Params Group Method: None\n",
      "2023-05-04 22:04:25,943 INFO    [optim_scheduler.py, 90] Use lambda_poly policy with default power 0.9\n",
      "2023-05-04 22:04:25,943 INFO    [optim_scheduler.py, 90] Use lambda_poly policy with default power 0.9\n",
      "2023-05-04 22:04:25,943 INFO    [data_loader.py, 131] use the DefaultLoader for train...\n",
      "2023-05-04 22:04:25,944 INFO    [data_loader.py, 131] use the DefaultLoader for train...\n",
      "2023-05-04 22:04:25,980 INFO    [data_loader.py, 164] use DefaultLoader for val ...\n",
      "2023-05-04 22:04:25,980 INFO    [data_loader.py, 164] use DefaultLoader for val ...\n",
      "2023-05-04 22:04:25,980 INFO    [data_loader.py, 164] use DefaultLoader for val ...\n",
      "2023-05-04 22:04:25,987 INFO    [loss_manager.py, 66] use loss: fs_auxce_loss.\n",
      "2023-05-04 22:04:25,987 INFO    [loss_manager.py, 66] use loss: fs_auxce_loss.\n",
      "2023-05-04 22:04:25,987 INFO    [loss_manager.py, 66] use loss: fs_auxce_loss.\n",
      "2023-05-04 22:04:25,987 INFO    [loss_manager.py, 47] use distributed loss\n",
      "2023-05-04 22:04:25,988 INFO    [loss_manager.py, 47] use distributed loss\n",
      "2023-05-04 22:04:25,988 INFO    [loss_manager.py, 47] use distributed loss\n",
      "2023-05-04 22:04:25,988 INFO    [data_loader.py, 164] use DefaultLoader for val ...\n",
      "2023-05-04 22:04:25,996 INFO    [loss_manager.py, 66] use loss: fs_auxce_loss.\n",
      "2023-05-04 22:04:25,997 INFO    [loss_manager.py, 47] use distributed loss\n",
      "2023-05-04 22:04:28,022 INFO    [trainer.py, 303] 0 images processed\n",
      "\n",
      "2023-05-04 22:04:28,022 INFO    [data_helper.py, 123] Input keys: ['img']\n",
      "2023-05-04 22:04:28,023 INFO    [data_helper.py, 124] Target keys: ['labelmap']\n",
      "2023-05-04 22:04:28,024 INFO    [trainer.py, 303] 0 images processed\n",
      "\n",
      "2023-05-04 22:04:28,025 INFO    [data_helper.py, 123] Input keys: ['img']\n",
      "2023-05-04 22:04:28,025 INFO    [data_helper.py, 124] Target keys: ['labelmap']\n",
      "2023-05-04 22:04:28,039 INFO    [trainer.py, 303] 0 images processed\n",
      "\n",
      "2023-05-04 22:04:28,040 INFO    [data_helper.py, 123] Input keys: ['img']\n",
      "2023-05-04 22:04:28,040 INFO    [data_helper.py, 124] Target keys: ['labelmap']\n",
      "2023-05-04 22:04:28,040 INFO    [trainer.py, 303] 0 images processed\n",
      "\n",
      "2023-05-04 22:04:28,041 INFO    [data_helper.py, 123] Input keys: ['img']\n",
      "2023-05-04 22:04:28,041 INFO    [data_helper.py, 124] Target keys: ['labelmap']\n",
      "/home/zk/anaconda3/envs/temp/lib/python3.7/site-packages/torch/nn/_reduction.py:13: UserWarning: reduction='elementwise_mean' is deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(\"reduction='elementwise_mean' is deprecated, please use reduction='mean' instead.\")\n",
      "/home/zk/anaconda3/envs/temp/lib/python3.7/site-packages/torch/nn/_reduction.py:13: UserWarning: reduction='elementwise_mean' is deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(\"reduction='elementwise_mean' is deprecated, please use reduction='mean' instead.\")\n",
      "/home/zk/anaconda3/envs/temp/lib/python3.7/site-packages/torch/nn/_reduction.py:13: UserWarning: reduction='elementwise_mean' is deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(\"reduction='elementwise_mean' is deprecated, please use reduction='mean' instead.\")\n",
      "/home/zk/anaconda3/envs/temp/lib/python3.7/site-packages/torch/nn/_reduction.py:13: UserWarning: reduction='elementwise_mean' is deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(\"reduction='elementwise_mean' is deprecated, please use reduction='mean' instead.\")\n",
      "2023-05-04 22:04:46,344 INFO    [trainer.py, 303] 10 images processed\n",
      "\n",
      "2023-05-04 22:04:46,344 INFO    [trainer.py, 303] 10 images processed\n",
      "\n",
      "2023-05-04 22:04:46,344 INFO    [trainer.py, 303] 10 images processed\n",
      "\n",
      "2023-05-04 22:04:46,344 INFO    [trainer.py, 303] 10 images processed\n",
      "\n",
      "2023-05-04 22:04:57,626 INFO    [trainer.py, 303] 20 images processed\n",
      "\n",
      "2023-05-04 22:04:57,626 INFO    [trainer.py, 303] 20 images processed\n",
      "\n",
      "2023-05-04 22:04:57,626 INFO    [trainer.py, 303] 20 images processed\n",
      "\n",
      "2023-05-04 22:04:57,626 INFO    [trainer.py, 303] 20 images processed\n",
      "\n",
      "2023-05-04 22:05:08,890 INFO    [trainer.py, 303] 30 images processed\n",
      "\n",
      "2023-05-04 22:05:08,890 INFO    [trainer.py, 303] 30 images processed\n",
      "\n",
      "2023-05-04 22:05:08,890 INFO    [trainer.py, 303] 30 images processed\n",
      "\n",
      "2023-05-04 22:05:08,890 INFO    [trainer.py, 303] 30 images processed\n",
      "\n",
      "2023-05-04 22:05:20,153 INFO    [trainer.py, 303] 40 images processed\n",
      "\n",
      "2023-05-04 22:05:20,153 INFO    [trainer.py, 303] 40 images processed\n",
      "\n",
      "2023-05-04 22:05:20,153 INFO    [trainer.py, 303] 40 images processed\n",
      "\n",
      "2023-05-04 22:05:20,153 INFO    [trainer.py, 303] 40 images processed\n",
      "\n",
      "2023-05-04 22:05:31,451 INFO    [trainer.py, 303] 50 images processed\n",
      "\n",
      "2023-05-04 22:05:31,451 INFO    [trainer.py, 303] 50 images processed\n",
      "\n",
      "2023-05-04 22:05:31,451 INFO    [trainer.py, 303] 50 images processed\n",
      "\n",
      "2023-05-04 22:05:31,451 INFO    [trainer.py, 303] 50 images processed\n",
      "\n",
      "2023-05-04 22:05:42,737 INFO    [trainer.py, 303] 60 images processed\n",
      "\n",
      "2023-05-04 22:05:42,737 INFO    [trainer.py, 303] 60 images processed\n",
      "\n",
      "2023-05-04 22:05:42,737 INFO    [trainer.py, 303] 60 images processed\n",
      "\n",
      "2023-05-04 22:05:42,737 INFO    [trainer.py, 303] 60 images processed\n",
      "\n",
      "2023-05-04 22:05:54,005 INFO    [trainer.py, 303] 70 images processed\n",
      "\n",
      "2023-05-04 22:05:54,005 INFO    [trainer.py, 303] 70 images processed\n",
      "\n",
      "2023-05-04 22:05:54,005 INFO    [trainer.py, 303] 70 images processed\n",
      "\n",
      "2023-05-04 22:05:54,005 INFO    [trainer.py, 303] 70 images processed\n",
      "\n",
      "2023-05-04 22:06:05,270 INFO    [trainer.py, 303] 80 images processed\n",
      "\n",
      "2023-05-04 22:06:05,270 INFO    [trainer.py, 303] 80 images processed\n",
      "\n",
      "2023-05-04 22:06:05,270 INFO    [trainer.py, 303] 80 images processed\n",
      "\n",
      "2023-05-04 22:06:05,270 INFO    [trainer.py, 303] 80 images processed\n",
      "\n",
      "2023-05-04 22:06:16,558 INFO    [trainer.py, 303] 90 images processed\n",
      "\n",
      "2023-05-04 22:06:16,558 INFO    [trainer.py, 303] 90 images processed\n",
      "\n",
      "2023-05-04 22:06:16,558 INFO    [trainer.py, 303] 90 images processed\n",
      "\n",
      "2023-05-04 22:06:16,558 INFO    [trainer.py, 303] 90 images processed\n",
      "\n",
      "2023-05-04 22:06:27,889 INFO    [trainer.py, 303] 100 images processed\n",
      "\n",
      "2023-05-04 22:06:27,889 INFO    [trainer.py, 303] 100 images processed\n",
      "\n",
      "2023-05-04 22:06:27,889 INFO    [trainer.py, 303] 100 images processed\n",
      "\n",
      "2023-05-04 22:06:27,889 INFO    [trainer.py, 303] 100 images processed\n",
      "\n",
      "2023-05-04 22:06:39,187 INFO    [trainer.py, 303] 110 images processed\n",
      "\n",
      "2023-05-04 22:06:39,187 INFO    [trainer.py, 303] 110 images processed\n",
      "\n",
      "2023-05-04 22:06:39,187 INFO    [trainer.py, 303] 110 images processed\n",
      "\n",
      "2023-05-04 22:06:39,187 INFO    [trainer.py, 303] 110 images processed\n",
      "\n",
      "2023-05-04 22:06:50,457 INFO    [trainer.py, 303] 120 images processed\n",
      "\n",
      "2023-05-04 22:06:50,457 INFO    [trainer.py, 303] 120 images processed\n",
      "\n",
      "2023-05-04 22:06:50,457 INFO    [trainer.py, 303] 120 images processed\n",
      "\n",
      "2023-05-04 22:06:50,457 INFO    [trainer.py, 303] 120 images processed\n",
      "\n",
      "2023-05-04 22:06:56,217 INFO    [base.py, 84] Performance 0.0 -> 0.004996691243704487\n",
      "2023-05-04 22:06:59,097 INFO    [trainer.py, 405] Test Time 150.117s, (1.201)\tLoss 4.13464462\n",
      "\n",
      "2023-05-04 22:06:59,098 INFO    [base.py, 33] Result for seg\n",
      "2023-05-04 22:06:59,098 INFO    [base.py, 49] Mean IOU: 0.004996691243704487\n",
      "\n",
      "2023-05-04 22:06:59,098 INFO    [base.py, 50] Pixel ACC: 0.01765222423994114\n",
      "\n",
      "/home/zk/anaconda3/envs/temp/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
      "/home/zk/anaconda3/envs/temp/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "/home/zk/anaconda3/envs/temp/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
      "/home/zk/anaconda3/envs/temp/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "/home/zk/anaconda3/envs/temp/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
      "/home/zk/anaconda3/envs/temp/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "/home/zk/anaconda3/envs/temp/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
      "/home/zk/anaconda3/envs/temp/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "2023-05-04 22:07:09,946 INFO    Reducer buckets have been rebuilt in this iteration.\n",
      "2023-05-04 22:07:09,946 INFO    Reducer buckets have been rebuilt in this iteration.\n",
      "2023-05-04 22:07:09,946 INFO    Reducer buckets have been rebuilt in this iteration.\n",
      "2023-05-04 22:07:09,947 INFO    Reducer buckets have been rebuilt in this iteration.\n",
      "2023-05-04 22:07:28,564 INFO    [trainer.py, 252] Train Epoch: 0\tTrain Iteration: 10\tTime 29.462s / 10iters, (2.946)\tForward Time 8.194s / 10iters, (0.819)\tBackward Time 20.345s / 10iters, (2.035)\tLoss Time 0.071s / 10iters, (0.007)\tData load 0.851s / 10iters, (0.085069)\n",
      "Learning rate = [0.00999797497721687, 0.00999797497721687]\tLoss = 2.96026373 (ave = 2.92111061)\n",
      "\n",
      "2023-05-04 22:07:48,898 INFO    [trainer.py, 252] Train Epoch: 0\tTrain Iteration: 20\tTime 20.334s / 10iters, (2.033)\tForward Time 5.046s / 10iters, (0.505)\tBackward Time 15.127s / 10iters, (1.513)\tLoss Time 0.075s / 10iters, (0.007)\tData load 0.086s / 10iters, (0.008607)\n",
      "Learning rate = [0.009995724898451063, 0.009995724898451063]\tLoss = 2.29443264 (ave = 1.90432318)\n",
      "\n",
      "2023-05-04 22:08:09,315 INFO    [trainer.py, 252] Train Epoch: 0\tTrain Iteration: 30\tTime 20.417s / 10iters, (2.042)\tForward Time 5.128s / 10iters, (0.513)\tBackward Time 15.124s / 10iters, (1.512)\tLoss Time 0.074s / 10iters, (0.007)\tData load 0.091s / 10iters, (0.009122)\n",
      "Learning rate = [0.00999347476340585, 0.00999347476340585]\tLoss = 1.17507708 (ave = 1.44081903)\n",
      "\n",
      "2023-05-04 22:08:29,519 INFO    [trainer.py, 252] Train Epoch: 0\tTrain Iteration: 40\tTime 20.203s / 10iters, (2.020)\tForward Time 5.090s / 10iters, (0.509)\tBackward Time 14.961s / 10iters, (1.496)\tLoss Time 0.071s / 10iters, (0.007)\tData load 0.082s / 10iters, (0.008155)\n",
      "Learning rate = [0.00999122457206574, 0.00999122457206574]\tLoss = 1.14194822 (ave = 1.48050244)\n",
      "\n",
      "2023-05-04 22:08:50,128 INFO    [trainer.py, 252] Train Epoch: 0\tTrain Iteration: 50\tTime 20.610s / 10iters, (2.061)\tForward Time 5.159s / 10iters, (0.516)\tBackward Time 15.307s / 10iters, (1.531)\tLoss Time 0.071s / 10iters, (0.007)\tData load 0.072s / 10iters, (0.007214)\n",
      "Learning rate = [0.00998897432441524, 0.00998897432441524]\tLoss = 1.34653652 (ave = 1.78818429)\n",
      "\n",
      "2023-05-04 22:09:10,589 INFO    [trainer.py, 252] Train Epoch: 0\tTrain Iteration: 60\tTime 20.461s / 10iters, (2.046)\tForward Time 5.025s / 10iters, (0.502)\tBackward Time 15.286s / 10iters, (1.529)\tLoss Time 0.075s / 10iters, (0.008)\tData load 0.075s / 10iters, (0.007483)\n",
      "Learning rate = [0.009986724020438846, 0.009986724020438846]\tLoss = 1.39944005 (ave = 1.10763033)\n",
      "\n",
      "2023-05-04 22:09:31,014 INFO    [trainer.py, 252] Train Epoch: 0\tTrain Iteration: 70\tTime 20.425s / 10iters, (2.043)\tForward Time 5.084s / 10iters, (0.508)\tBackward Time 15.183s / 10iters, (1.518)\tLoss Time 0.072s / 10iters, (0.007)\tData load 0.086s / 10iters, (0.008627)\n",
      "Learning rate = [0.009984473660121045, 0.009984473660121045]\tLoss = 1.43716908 (ave = 1.12670883)\n",
      "\n",
      "2023-05-04 22:09:51,437 INFO    [trainer.py, 252] Train Epoch: 0\tTrain Iteration: 80\tTime 20.423s / 10iters, (2.042)\tForward Time 5.056s / 10iters, (0.506)\tBackward Time 15.207s / 10iters, (1.521)\tLoss Time 0.073s / 10iters, (0.007)\tData load 0.085s / 10iters, (0.008547)\n",
      "Learning rate = [0.009982223243446315, 0.009982223243446315]\tLoss = 0.88370687 (ave = 1.04337461)\n",
      "\n",
      "2023-05-04 22:10:11,907 INFO    [trainer.py, 252] Train Epoch: 0\tTrain Iteration: 90\tTime 20.470s / 10iters, (2.047)\tForward Time 5.173s / 10iters, (0.517)\tBackward Time 15.145s / 10iters, (1.515)\tLoss Time 0.072s / 10iters, (0.007)\tData load 0.081s / 10iters, (0.008073)\n",
      "Learning rate = [0.009979972770399127, 0.009979972770399127]\tLoss = 0.81966656 (ave = 1.17333472)\n",
      "\n",
      "2023-05-04 22:10:32,405 INFO    [trainer.py, 252] Train Epoch: 0\tTrain Iteration: 100\tTime 20.498s / 10iters, (2.050)\tForward Time 5.192s / 10iters, (0.519)\tBackward Time 15.149s / 10iters, (1.515)\tLoss Time 0.075s / 10iters, (0.007)\tData load 0.082s / 10iters, (0.008169)\n",
      "Learning rate = [0.009977722240963943, 0.009977722240963943]\tLoss = 1.05245304 (ave = 0.94993327)\n",
      "\n",
      "2023-05-04 22:10:52,694 INFO    [trainer.py, 252] Train Epoch: 0\tTrain Iteration: 110\tTime 20.289s / 10iters, (2.029)\tForward Time 5.118s / 10iters, (0.512)\tBackward Time 15.019s / 10iters, (1.502)\tLoss Time 0.071s / 10iters, (0.007)\tData load 0.081s / 10iters, (0.008090)\n",
      "Learning rate = [0.00997547165512522, 0.00997547165512522]\tLoss = 0.97930551 (ave = 1.00702273)\n",
      "\n",
      "2023-05-04 22:11:13,165 INFO    [trainer.py, 252] Train Epoch: 0\tTrain Iteration: 120\tTime 20.470s / 10iters, (2.047)\tForward Time 5.111s / 10iters, (0.511)\tBackward Time 15.207s / 10iters, (1.521)\tLoss Time 0.074s / 10iters, (0.007)\tData load 0.079s / 10iters, (0.007857)\n",
      "Learning rate = [0.009973221012867402, 0.009973221012867402]\tLoss = 0.86095667 (ave = 1.08557991)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!bash experiment/mep.sh train"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "42a4ea106d429adb85b92cbf5fe6fad5ad2431a3e82cdef4435b2cc92f522609"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
