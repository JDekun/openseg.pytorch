{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 41 µs, total: 41 µs\n",
      "Wall time: 53.2 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import os \n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./log/cityscapes/dc_spatial_ocrnet_dc_deepbase_resnet101_dilated8_2023-04-26_12-00-55.log\n",
      "World size: 4\n",
      "['--configs', 'configs/cityscapes/DC_R_101_D_8.json', '--drop_last', 'y', '--phase', 'train', '--gathered', 'n', '--loss_balance', 'y', '--log_to_file', 'n', '--backbone', 'deepbase_resnet101_dilated8', '--model_name', 'spatial_ocrnet_dc', '--workers', '4', '--gpu', '2', '3', '4', '6', '--train_batch_size', '8', '--val_batch_size', '4', '--data_dir', '../../input/openseg-cityscapes-gtfine', '--loss_type', 'fs_auxce_loss_dc', '--max_iters', '40000', '--checkpoints_name', 'dc_spatial_ocrnet_dc_deepbase_resnet101_dilated8_2023-04-26_12-00-55', '--pretrained', '../../input/pre-trained/resnet101-imagenet-openseg.pth', '--distributed']\n",
      "/home/zk/anaconda3/envs/temp/lib/python3.7/site-packages/torch/distributed/launch.py:186: FutureWarning: The module torch.distributed.launch is deprecated\n",
      "and will be removed in future. Use torchrun.\n",
      "Note that --use_env is set by default in torchrun.\n",
      "If your script expects `--local_rank` argument to be set, please\n",
      "change it to read from `os.environ['LOCAL_RANK']` instead. See \n",
      "https://pytorch.org/docs/stable/distributed.html#launch-utility for \n",
      "further instructions\n",
      "\n",
      "  FutureWarning,\n",
      "WARNING:torch.distributed.run:\n",
      "*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "2023-04-26 12:01:00,459 INFO    [offset_helper.py, 54] engery/max-distance: 5 engery/min-distance: 0\n",
      "2023-04-26 12:01:00,459 INFO    [offset_helper.py, 61] direction/num_classes: 8 scale: 1\n",
      "2023-04-26 12:01:00,459 INFO    [offset_helper.py, 66] c4 align axis: False\n",
      "2023-04-26 12:01:00,465 INFO    [offset_helper.py, 54] engery/max-distance: 5 engery/min-distance: 0\n",
      "2023-04-26 12:01:00,466 INFO    [offset_helper.py, 61] direction/num_classes: 8 scale: 1\n",
      "2023-04-26 12:01:00,466 INFO    [offset_helper.py, 66] c4 align axis: False\n",
      "2023-04-26 12:01:00,475 INFO    [module_runner.py, 44] BN Type is torchsyncbn.\n",
      "2023-04-26 12:01:00,475 INFO    [__init__.py, 17] Using evaluator: StandardEvaluator\n",
      "2023-04-26 12:01:00,482 INFO    [module_runner.py, 44] BN Type is torchsyncbn.\n",
      "2023-04-26 12:01:00,482 INFO    [__init__.py, 17] Using evaluator: StandardEvaluator\n",
      "2023-04-26 12:01:00,488 INFO    [offset_helper.py, 54] engery/max-distance: 5 engery/min-distance: 0\n",
      "2023-04-26 12:01:00,488 INFO    [offset_helper.py, 61] direction/num_classes: 8 scale: 1\n",
      "2023-04-26 12:01:00,488 INFO    [offset_helper.py, 66] c4 align axis: False\n",
      "2023-04-26 12:01:00,502 INFO    [offset_helper.py, 54] engery/max-distance: 5 engery/min-distance: 0\n",
      "2023-04-26 12:01:00,503 INFO    [offset_helper.py, 61] direction/num_classes: 8 scale: 1\n",
      "2023-04-26 12:01:00,503 INFO    [module_runner.py, 44] BN Type is torchsyncbn.\n",
      "2023-04-26 12:01:00,503 INFO    [offset_helper.py, 66] c4 align axis: False\n",
      "2023-04-26 12:01:00,503 INFO    [__init__.py, 17] Using evaluator: StandardEvaluator\n",
      "2023-04-26 12:01:00,519 INFO    [module_runner.py, 44] BN Type is torchsyncbn.\n",
      "2023-04-26 12:01:00,519 INFO    [__init__.py, 17] Using evaluator: StandardEvaluator\n",
      "2023-04-26 12:01:01,385 INFO    [module_helper.py, 128] Loading pretrained model:../../input/pre-trained/resnet101-imagenet-openseg.pth\n",
      "2023-04-26 12:01:01,407 INFO    [module_helper.py, 128] Loading pretrained model:../../input/pre-trained/resnet101-imagenet-openseg.pth\n",
      "2023-04-26 12:01:01,416 INFO    [module_helper.py, 128] Loading pretrained model:../../input/pre-trained/resnet101-imagenet-openseg.pth\n",
      "2023-04-26 12:01:01,518 INFO    [module_helper.py, 128] Loading pretrained model:../../input/pre-trained/resnet101-imagenet-openseg.pth\n",
      "2023-04-26 12:01:11,009 INFO    [trainer.py, 78] Params Group Method: None\n",
      "2023-04-26 12:01:11,010 INFO    [trainer.py, 78] Params Group Method: None\n",
      "2023-04-26 12:01:11,011 INFO    [trainer.py, 78] Params Group Method: None\n",
      "2023-04-26 12:01:11,012 INFO    [trainer.py, 78] Params Group Method: None\n",
      "2023-04-26 12:01:11,013 INFO    [optim_scheduler.py, 90] Use lambda_poly policy with default power 0.9\n",
      "2023-04-26 12:01:11,013 INFO    [data_loader.py, 131] use the DefaultLoader for train...\n",
      "2023-04-26 12:01:11,015 INFO    [optim_scheduler.py, 90] Use lambda_poly policy with default power 0.9\n",
      "2023-04-26 12:01:11,015 INFO    [data_loader.py, 131] use the DefaultLoader for train...\n",
      "2023-04-26 12:01:11,015 INFO    [optim_scheduler.py, 90] Use lambda_poly policy with default power 0.9\n",
      "2023-04-26 12:01:11,016 INFO    [data_loader.py, 131] use the DefaultLoader for train...\n",
      "2023-04-26 12:01:11,016 INFO    [optim_scheduler.py, 90] Use lambda_poly policy with default power 0.9\n",
      "2023-04-26 12:01:11,016 INFO    [data_loader.py, 131] use the DefaultLoader for train...\n",
      "2023-04-26 12:01:11,052 INFO    [data_loader.py, 164] use DefaultLoader for val ...\n",
      "2023-04-26 12:01:11,052 INFO    [data_loader.py, 164] use DefaultLoader for val ...\n",
      "2023-04-26 12:01:11,055 INFO    [data_loader.py, 164] use DefaultLoader for val ...\n",
      "2023-04-26 12:01:11,056 INFO    [data_loader.py, 164] use DefaultLoader for val ...\n",
      "2023-04-26 12:01:11,060 INFO    [loss_manager.py, 64] use loss: fs_auxce_loss_dc.\n",
      "2023-04-26 12:01:11,060 INFO    [loss_manager.py, 45] use distributed loss\n",
      "2023-04-26 12:01:11,061 INFO    [loss_manager.py, 64] use loss: fs_auxce_loss_dc.\n",
      "2023-04-26 12:01:11,062 INFO    [loss_manager.py, 64] use loss: fs_auxce_loss_dc.\n",
      "2023-04-26 12:01:11,062 INFO    [loss_manager.py, 45] use distributed loss\n",
      "2023-04-26 12:01:11,063 INFO    [loss_manager.py, 45] use distributed loss\n",
      "2023-04-26 12:01:11,064 INFO    [loss_manager.py, 64] use loss: fs_auxce_loss_dc.\n",
      "2023-04-26 12:01:11,064 INFO    [loss_manager.py, 45] use distributed loss\n",
      "2023-04-26 12:01:11,794 INFO    [trainer.py, 301] 0 images processed\n",
      "\n",
      "2023-04-26 12:01:11,794 INFO    [trainer.py, 301] 0 images processed\n",
      "\n",
      "2023-04-26 12:01:11,795 INFO    [data_helper.py, 123] Input keys: ['img']\n",
      "2023-04-26 12:01:11,795 INFO    [data_helper.py, 123] Input keys: ['img']\n",
      "2023-04-26 12:01:11,795 INFO    [data_helper.py, 124] Target keys: ['labelmap']\n",
      "2023-04-26 12:01:11,795 INFO    [data_helper.py, 124] Target keys: ['labelmap']\n",
      "2023-04-26 12:01:11,802 INFO    [trainer.py, 301] 0 images processed\n",
      "\n",
      "2023-04-26 12:01:11,803 INFO    [data_helper.py, 123] Input keys: ['img']\n",
      "2023-04-26 12:01:11,803 INFO    [data_helper.py, 124] Target keys: ['labelmap']\n",
      "2023-04-26 12:01:11,804 INFO    [trainer.py, 301] 0 images processed\n",
      "\n",
      "2023-04-26 12:01:11,805 INFO    [data_helper.py, 123] Input keys: ['img']\n",
      "2023-04-26 12:01:11,805 INFO    [data_helper.py, 124] Target keys: ['labelmap']\n",
      "/home/zk/anaconda3/envs/temp/lib/python3.7/site-packages/torch/nn/_reduction.py:13: UserWarning: reduction='elementwise_mean' is deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(\"reduction='elementwise_mean' is deprecated, please use reduction='mean' instead.\")\n",
      "/home/zk/anaconda3/envs/temp/lib/python3.7/site-packages/torch/nn/_reduction.py:13: UserWarning: reduction='elementwise_mean' is deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(\"reduction='elementwise_mean' is deprecated, please use reduction='mean' instead.\")\n",
      "/home/zk/anaconda3/envs/temp/lib/python3.7/site-packages/torch/nn/_reduction.py:13: UserWarning: reduction='elementwise_mean' is deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(\"reduction='elementwise_mean' is deprecated, please use reduction='mean' instead.\")\n",
      "/home/zk/anaconda3/envs/temp/lib/python3.7/site-packages/torch/nn/_reduction.py:13: UserWarning: reduction='elementwise_mean' is deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(\"reduction='elementwise_mean' is deprecated, please use reduction='mean' instead.\")\n",
      "2023-04-26 12:01:28,577 INFO    [trainer.py, 301] 10 images processed\n",
      "\n",
      "2023-04-26 12:01:28,577 INFO    [trainer.py, 301] 10 images processed\n",
      "\n",
      "2023-04-26 12:01:28,577 INFO    [trainer.py, 301] 10 images processed\n",
      "\n",
      "2023-04-26 12:01:28,577 INFO    [trainer.py, 301] 10 images processed\n",
      "\n",
      "2023-04-26 12:01:39,571 INFO    [trainer.py, 301] 20 images processed\n",
      "\n",
      "2023-04-26 12:01:39,571 INFO    [trainer.py, 301] 20 images processed\n",
      "\n",
      "2023-04-26 12:01:39,571 INFO    [trainer.py, 301] 20 images processed\n",
      "\n",
      "2023-04-26 12:01:39,572 INFO    [trainer.py, 301] 20 images processed\n",
      "\n",
      "2023-04-26 12:01:50,372 INFO    [trainer.py, 301] 30 images processed\n",
      "\n",
      "2023-04-26 12:01:50,372 INFO    [trainer.py, 301] 30 images processed\n",
      "\n",
      "2023-04-26 12:01:50,372 INFO    [trainer.py, 301] 30 images processed\n",
      "\n",
      "2023-04-26 12:01:50,372 INFO    [trainer.py, 301] 30 images processed\n",
      "\n",
      "2023-04-26 12:02:01,208 INFO    [trainer.py, 301] 40 images processed\n",
      "\n",
      "2023-04-26 12:02:01,208 INFO    [trainer.py, 301] 40 images processed\n",
      "\n",
      "2023-04-26 12:02:01,208 INFO    [trainer.py, 301] 40 images processed\n",
      "\n",
      "2023-04-26 12:02:01,208 INFO    [trainer.py, 301] 40 images processed\n",
      "\n",
      "2023-04-26 12:02:12,011 INFO    [trainer.py, 301] 50 images processed\n",
      "\n",
      "2023-04-26 12:02:12,011 INFO    [trainer.py, 301] 50 images processed\n",
      "\n",
      "2023-04-26 12:02:12,011 INFO    [trainer.py, 301] 50 images processed\n",
      "\n",
      "2023-04-26 12:02:12,011 INFO    [trainer.py, 301] 50 images processed\n",
      "\n",
      "2023-04-26 12:02:22,887 INFO    [trainer.py, 301] 60 images processed\n",
      "\n",
      "2023-04-26 12:02:22,887 INFO    [trainer.py, 301] 60 images processed\n",
      "\n",
      "2023-04-26 12:02:22,887 INFO    [trainer.py, 301] 60 images processed\n",
      "\n",
      "2023-04-26 12:02:22,887 INFO    [trainer.py, 301] 60 images processed\n",
      "\n",
      "2023-04-26 12:02:33,737 INFO    [trainer.py, 301] 70 images processed\n",
      "\n",
      "2023-04-26 12:02:33,737 INFO    [trainer.py, 301] 70 images processed\n",
      "\n",
      "2023-04-26 12:02:33,737 INFO    [trainer.py, 301] 70 images processed\n",
      "\n",
      "2023-04-26 12:02:33,737 INFO    [trainer.py, 301] 70 images processed\n",
      "\n",
      "2023-04-26 12:02:44,598 INFO    [trainer.py, 301] 80 images processed\n",
      "\n",
      "2023-04-26 12:02:44,598 INFO    [trainer.py, 301] 80 images processed\n",
      "\n",
      "2023-04-26 12:02:44,598 INFO    [trainer.py, 301] 80 images processed\n",
      "\n",
      "2023-04-26 12:02:44,598 INFO    [trainer.py, 301] 80 images processed\n",
      "\n",
      "2023-04-26 12:02:55,393 INFO    [trainer.py, 301] 90 images processed\n",
      "\n",
      "2023-04-26 12:02:55,393 INFO    [trainer.py, 301] 90 images processed\n",
      "\n",
      "2023-04-26 12:02:55,393 INFO    [trainer.py, 301] 90 images processed\n",
      "\n",
      "2023-04-26 12:02:55,393 INFO    [trainer.py, 301] 90 images processed\n",
      "\n",
      "2023-04-26 12:03:06,201 INFO    [trainer.py, 301] 100 images processed\n",
      "\n",
      "2023-04-26 12:03:06,201 INFO    [trainer.py, 301] 100 images processed\n",
      "\n",
      "2023-04-26 12:03:06,201 INFO    [trainer.py, 301] 100 images processed\n",
      "\n",
      "2023-04-26 12:03:06,201 INFO    [trainer.py, 301] 100 images processed\n",
      "\n",
      "2023-04-26 12:03:16,926 INFO    [trainer.py, 301] 110 images processed\n",
      "\n",
      "2023-04-26 12:03:16,926 INFO    [trainer.py, 301] 110 images processed\n",
      "\n",
      "2023-04-26 12:03:16,926 INFO    [trainer.py, 301] 110 images processed\n",
      "\n",
      "2023-04-26 12:03:16,926 INFO    [trainer.py, 301] 110 images processed\n",
      "\n",
      "2023-04-26 12:03:27,712 INFO    [trainer.py, 301] 120 images processed\n",
      "\n",
      "2023-04-26 12:03:27,712 INFO    [trainer.py, 301] 120 images processed\n",
      "\n",
      "2023-04-26 12:03:27,712 INFO    [trainer.py, 301] 120 images processed\n",
      "\n",
      "2023-04-26 12:03:27,712 INFO    [trainer.py, 301] 120 images processed\n",
      "\n",
      "2023-04-26 12:03:33,148 INFO    [base.py, 84] Performance 0.0 -> 0.003599200480956761\n",
      "2023-04-26 12:03:35,704 INFO    [trainer.py, 403] Test Time 141.864s, (1.135)\tLoss 5.44022427\n",
      "\n",
      "2023-04-26 12:03:35,705 INFO    [base.py, 33] Result for seg\n",
      "2023-04-26 12:03:35,706 INFO    [base.py, 49] Mean IOU: 0.003599200480956761\n",
      "\n",
      "2023-04-26 12:03:35,706 INFO    [base.py, 50] Pixel ACC: 0.018252834812799504\n",
      "\n",
      "/home/zk/anaconda3/envs/temp/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
      "/home/zk/anaconda3/envs/temp/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "/home/zk/anaconda3/envs/temp/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
      "/home/zk/anaconda3/envs/temp/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "/home/zk/anaconda3/envs/temp/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
      "/home/zk/anaconda3/envs/temp/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
      "/home/zk/anaconda3/envs/temp/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "/home/zk/anaconda3/envs/temp/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "2023-04-26 12:03:45,800 INFO    Reducer buckets have been rebuilt in this iteration.\n",
      "2023-04-26 12:03:45,800 INFO    Reducer buckets have been rebuilt in this iteration.\n",
      "2023-04-26 12:03:45,800 INFO    Reducer buckets have been rebuilt in this iteration.\n",
      "2023-04-26 12:03:45,800 INFO    Reducer buckets have been rebuilt in this iteration.\n",
      "2023-04-26 12:04:05,388 INFO    [trainer.py, 250] Train Epoch: 0\tTrain Iteration: 10\tTime 29.676s / 10iters, (2.968)\tForward Time 8.238s / 10iters, (0.824)\tBackward Time 19.383s / 10iters, (1.938)\tLoss Time 1.384s / 10iters, (0.138)\tData load 0.671s / 10iters, (0.067085)\n",
      "Learning rate = [0.00999797497721687, 0.00999797497721687]\tLoss = 3.48282433 (ave = 4.71057436)\n",
      "\n",
      "2023-04-26 12:04:25,709 INFO    [trainer.py, 250] Train Epoch: 0\tTrain Iteration: 20\tTime 20.322s / 10iters, (2.032)\tForward Time 4.766s / 10iters, (0.477)\tBackward Time 14.501s / 10iters, (1.450)\tLoss Time 0.977s / 10iters, (0.098)\tData load 0.078s / 10iters, (0.007768)\n",
      "Learning rate = [0.009995724898451063, 0.009995724898451063]\tLoss = 3.84522486 (ave = 3.70245168)\n",
      "\n",
      "2023-04-26 12:04:45,370 INFO    [trainer.py, 250] Train Epoch: 0\tTrain Iteration: 30\tTime 19.661s / 10iters, (1.966)\tForward Time 4.927s / 10iters, (0.493)\tBackward Time 13.839s / 10iters, (1.384)\tLoss Time 0.808s / 10iters, (0.081)\tData load 0.086s / 10iters, (0.008610)\n",
      "Learning rate = [0.00999347476340585, 0.00999347476340585]\tLoss = 3.91873646 (ave = 3.55460300)\n",
      "\n",
      "2023-04-26 12:05:05,427 INFO    [trainer.py, 250] Train Epoch: 0\tTrain Iteration: 40\tTime 20.057s / 10iters, (2.006)\tForward Time 4.836s / 10iters, (0.484)\tBackward Time 14.220s / 10iters, (1.422)\tLoss Time 0.918s / 10iters, (0.092)\tData load 0.083s / 10iters, (0.008259)\n",
      "Learning rate = [0.00999122457206574, 0.00999122457206574]\tLoss = 3.11690331 (ave = 3.43275888)\n",
      "\n",
      "2023-04-26 12:05:25,437 INFO    [trainer.py, 250] Train Epoch: 0\tTrain Iteration: 50\tTime 20.011s / 10iters, (2.001)\tForward Time 4.820s / 10iters, (0.482)\tBackward Time 14.196s / 10iters, (1.420)\tLoss Time 0.915s / 10iters, (0.092)\tData load 0.080s / 10iters, (0.007954)\n",
      "Learning rate = [0.00998897432441524, 0.00998897432441524]\tLoss = 2.90911174 (ave = 4.01000571)\n",
      "\n",
      "2023-04-26 12:05:45,717 INFO    [trainer.py, 250] Train Epoch: 0\tTrain Iteration: 60\tTime 20.280s / 10iters, (2.028)\tForward Time 4.742s / 10iters, (0.474)\tBackward Time 14.399s / 10iters, (1.440)\tLoss Time 1.055s / 10iters, (0.106)\tData load 0.084s / 10iters, (0.008379)\n",
      "Learning rate = [0.009986724020438846, 0.009986724020438846]\tLoss = 2.97305536 (ave = 2.95683696)\n",
      "\n",
      "2023-04-26 12:06:05,510 INFO    [trainer.py, 250] Train Epoch: 0\tTrain Iteration: 70\tTime 19.793s / 10iters, (1.979)\tForward Time 4.769s / 10iters, (0.477)\tBackward Time 13.973s / 10iters, (1.397)\tLoss Time 0.961s / 10iters, (0.096)\tData load 0.090s / 10iters, (0.008960)\n",
      "Learning rate = [0.009984473660121045, 0.009984473660121045]\tLoss = 2.55090189 (ave = 2.80722482)\n",
      "\n",
      "2023-04-26 12:06:25,774 INFO    [trainer.py, 250] Train Epoch: 0\tTrain Iteration: 80\tTime 20.264s / 10iters, (2.026)\tForward Time 4.792s / 10iters, (0.479)\tBackward Time 14.280s / 10iters, (1.428)\tLoss Time 1.110s / 10iters, (0.111)\tData load 0.082s / 10iters, (0.008168)\n",
      "Learning rate = [0.009982223243446315, 0.009982223243446315]\tLoss = 3.70714545 (ave = 3.07325058)\n",
      "\n",
      "2023-04-26 12:06:46,045 INFO    [trainer.py, 250] Train Epoch: 0\tTrain Iteration: 90\tTime 20.271s / 10iters, (2.027)\tForward Time 4.783s / 10iters, (0.478)\tBackward Time 14.446s / 10iters, (1.445)\tLoss Time 0.962s / 10iters, (0.096)\tData load 0.080s / 10iters, (0.007962)\n",
      "Learning rate = [0.009979972770399127, 0.009979972770399127]\tLoss = 2.46928453 (ave = 2.64296261)\n",
      "\n",
      "2023-04-26 12:07:05,701 INFO    [trainer.py, 250] Train Epoch: 0\tTrain Iteration: 100\tTime 19.656s / 10iters, (1.966)\tForward Time 4.793s / 10iters, (0.479)\tBackward Time 13.870s / 10iters, (1.387)\tLoss Time 0.909s / 10iters, (0.091)\tData load 0.084s / 10iters, (0.008356)\n",
      "Learning rate = [0.009977722240963943, 0.009977722240963943]\tLoss = 2.55427504 (ave = 2.51701964)\n",
      "\n",
      "2023-04-26 12:07:25,785 INFO    [trainer.py, 250] Train Epoch: 0\tTrain Iteration: 110\tTime 20.084s / 10iters, (2.008)\tForward Time 4.812s / 10iters, (0.481)\tBackward Time 14.116s / 10iters, (1.412)\tLoss Time 1.075s / 10iters, (0.108)\tData load 0.080s / 10iters, (0.008023)\n",
      "Learning rate = [0.00997547165512522, 0.00997547165512522]\tLoss = 2.97133780 (ave = 2.70910659)\n",
      "\n",
      "2023-04-26 12:07:45,854 INFO    [trainer.py, 250] Train Epoch: 0\tTrain Iteration: 120\tTime 20.069s / 10iters, (2.007)\tForward Time 4.759s / 10iters, (0.476)\tBackward Time 14.249s / 10iters, (1.425)\tLoss Time 0.979s / 10iters, (0.098)\tData load 0.081s / 10iters, (0.008137)\n",
      "Learning rate = [0.009973221012867402, 0.009973221012867402]\tLoss = 2.87705755 (ave = 2.50052357)\n",
      "\n",
      "2023-04-26 12:08:05,907 INFO    [trainer.py, 250] Train Epoch: 0\tTrain Iteration: 130\tTime 20.053s / 10iters, (2.005)\tForward Time 4.840s / 10iters, (0.484)\tBackward Time 14.125s / 10iters, (1.412)\tLoss Time 1.002s / 10iters, (0.100)\tData load 0.086s / 10iters, (0.008564)\n",
      "Learning rate = [0.009970970314174928, 0.009970970314174928]\tLoss = 4.07523155 (ave = 3.15911052)\n",
      "\n",
      "2023-04-26 12:08:25,398 INFO    [trainer.py, 250] Train Epoch: 0\tTrain Iteration: 140\tTime 19.491s / 10iters, (1.949)\tForward Time 4.730s / 10iters, (0.473)\tBackward Time 13.631s / 10iters, (1.363)\tLoss Time 1.044s / 10iters, (0.104)\tData load 0.086s / 10iters, (0.008645)\n",
      "Learning rate = [0.00996871955903223, 0.00996871955903223]\tLoss = 3.02849960 (ave = 2.57419884)\n",
      "\n",
      "2023-04-26 12:08:45,215 INFO    [trainer.py, 250] Train Epoch: 0\tTrain Iteration: 150\tTime 19.817s / 10iters, (1.982)\tForward Time 4.803s / 10iters, (0.480)\tBackward Time 13.752s / 10iters, (1.375)\tLoss Time 1.181s / 10iters, (0.118)\tData load 0.080s / 10iters, (0.008036)\n",
      "Learning rate = [0.009966468747423728, 0.009966468747423728]\tLoss = 2.18785143 (ave = 2.92002432)\n",
      "\n",
      "2023-04-26 12:09:04,875 INFO    [trainer.py, 250] Train Epoch: 0\tTrain Iteration: 160\tTime 19.660s / 10iters, (1.966)\tForward Time 4.831s / 10iters, (0.483)\tBackward Time 13.780s / 10iters, (1.378)\tLoss Time 0.965s / 10iters, (0.097)\tData load 0.083s / 10iters, (0.008311)\n",
      "Learning rate = [0.009964217879333836, 0.009964217879333836]\tLoss = 2.36328030 (ave = 2.66034596)\n",
      "\n",
      "2023-04-26 12:09:24,641 INFO    [trainer.py, 250] Train Epoch: 0\tTrain Iteration: 170\tTime 19.766s / 10iters, (1.977)\tForward Time 4.727s / 10iters, (0.473)\tBackward Time 13.838s / 10iters, (1.384)\tLoss Time 1.115s / 10iters, (0.112)\tData load 0.086s / 10iters, (0.008620)\n",
      "Learning rate = [0.009961966954746958, 0.009961966954746958]\tLoss = 2.29622316 (ave = 2.49288406)\n",
      "\n",
      "2023-04-26 12:09:44,456 INFO    [trainer.py, 250] Train Epoch: 0\tTrain Iteration: 180\tTime 19.815s / 10iters, (1.981)\tForward Time 4.808s / 10iters, (0.481)\tBackward Time 13.842s / 10iters, (1.384)\tLoss Time 1.081s / 10iters, (0.108)\tData load 0.084s / 10iters, (0.008419)\n",
      "Learning rate = [0.009959715973647493, 0.009959715973647493]\tLoss = 2.07756257 (ave = 2.64288274)\n",
      "\n",
      "2023-04-26 12:10:04,348 INFO    [trainer.py, 250] Train Epoch: 0\tTrain Iteration: 190\tTime 19.892s / 10iters, (1.989)\tForward Time 4.893s / 10iters, (0.489)\tBackward Time 13.924s / 10iters, (1.392)\tLoss Time 0.998s / 10iters, (0.100)\tData load 0.077s / 10iters, (0.007711)\n",
      "Learning rate = [0.00995746493601983, 0.00995746493601983]\tLoss = 2.29231071 (ave = 2.32854149)\n",
      "\n",
      "2023-04-26 12:10:23,901 INFO    [trainer.py, 250] Train Epoch: 0\tTrain Iteration: 200\tTime 19.553s / 10iters, (1.955)\tForward Time 4.834s / 10iters, (0.483)\tBackward Time 13.736s / 10iters, (1.374)\tLoss Time 0.900s / 10iters, (0.090)\tData load 0.083s / 10iters, (0.008296)\n",
      "Learning rate = [0.00995521384184835, 0.00995521384184835]\tLoss = 2.07569194 (ave = 2.14528111)\n",
      "\n",
      "2023-04-26 12:10:43,055 INFO    [trainer.py, 250] Train Epoch: 0\tTrain Iteration: 210\tTime 19.154s / 10iters, (1.915)\tForward Time 4.708s / 10iters, (0.471)\tBackward Time 13.284s / 10iters, (1.328)\tLoss Time 1.082s / 10iters, (0.108)\tData load 0.080s / 10iters, (0.008006)\n",
      "Learning rate = [0.009952962691117425, 0.009952962691117425]\tLoss = 1.69220805 (ave = 2.16834633)\n",
      "\n",
      "2023-04-26 12:11:02,258 INFO    [trainer.py, 250] Train Epoch: 0\tTrain Iteration: 220\tTime 19.203s / 10iters, (1.920)\tForward Time 4.861s / 10iters, (0.486)\tBackward Time 13.270s / 10iters, (1.327)\tLoss Time 0.993s / 10iters, (0.099)\tData load 0.079s / 10iters, (0.007858)\n",
      "Learning rate = [0.009950711483811419, 0.009950711483811419]\tLoss = 2.32171822 (ave = 2.03896840)\n",
      "\n",
      "2023-04-26 12:11:21,635 INFO    [trainer.py, 250] Train Epoch: 0\tTrain Iteration: 230\tTime 19.377s / 10iters, (1.938)\tForward Time 4.756s / 10iters, (0.476)\tBackward Time 13.459s / 10iters, (1.346)\tLoss Time 1.079s / 10iters, (0.108)\tData load 0.083s / 10iters, (0.008307)\n",
      "Learning rate = [0.009948460219914688, 0.009948460219914688]\tLoss = 2.68624878 (ave = 2.30755172)\n",
      "\n",
      "2023-04-26 12:11:41,193 INFO    [trainer.py, 250] Train Epoch: 0\tTrain Iteration: 240\tTime 19.558s / 10iters, (1.956)\tForward Time 4.761s / 10iters, (0.476)\tBackward Time 13.590s / 10iters, (1.359)\tLoss Time 1.126s / 10iters, (0.113)\tData load 0.080s / 10iters, (0.008003)\n",
      "Learning rate = [0.00994620889941158, 0.00994620889941158]\tLoss = 2.34644556 (ave = 2.28956631)\n",
      "\n",
      "2023-04-26 12:12:00,540 INFO    [trainer.py, 250] Train Epoch: 0\tTrain Iteration: 250\tTime 19.348s / 10iters, (1.935)\tForward Time 4.816s / 10iters, (0.482)\tBackward Time 13.521s / 10iters, (1.352)\tLoss Time 0.924s / 10iters, (0.092)\tData load 0.087s / 10iters, (0.008687)\n",
      "Learning rate = [0.009943957522286433, 0.009943957522286433]\tLoss = 1.97051311 (ave = 1.96892791)\n",
      "\n",
      "2023-04-26 12:12:19,883 INFO    [trainer.py, 250] Train Epoch: 0\tTrain Iteration: 260\tTime 19.342s / 10iters, (1.934)\tForward Time 4.864s / 10iters, (0.486)\tBackward Time 13.326s / 10iters, (1.333)\tLoss Time 1.076s / 10iters, (0.108)\tData load 0.076s / 10iters, (0.007623)\n",
      "Learning rate = [0.00994170608852358, 0.00994170608852358]\tLoss = 1.95882440 (ave = 2.26648121)\n",
      "\n",
      "2023-04-26 12:12:39,261 INFO    [trainer.py, 250] Train Epoch: 0\tTrain Iteration: 270\tTime 19.379s / 10iters, (1.938)\tForward Time 4.782s / 10iters, (0.478)\tBackward Time 13.492s / 10iters, (1.349)\tLoss Time 1.024s / 10iters, (0.102)\tData load 0.080s / 10iters, (0.008039)\n",
      "Learning rate = [0.009939454598107345, 0.009939454598107345]\tLoss = 2.14162397 (ave = 2.09455816)\n",
      "\n",
      "2023-04-26 12:12:58,389 INFO    [trainer.py, 250] Train Epoch: 0\tTrain Iteration: 280\tTime 19.127s / 10iters, (1.913)\tForward Time 4.793s / 10iters, (0.479)\tBackward Time 13.251s / 10iters, (1.325)\tLoss Time 1.002s / 10iters, (0.100)\tData load 0.081s / 10iters, (0.008058)\n",
      "Learning rate = [0.00993720305102204, 0.00993720305102204]\tLoss = 1.64411533 (ave = 2.01238689)\n",
      "\n",
      "2023-04-26 12:13:17,464 INFO    [trainer.py, 250] Train Epoch: 0\tTrain Iteration: 290\tTime 19.075s / 10iters, (1.908)\tForward Time 4.697s / 10iters, (0.470)\tBackward Time 13.116s / 10iters, (1.312)\tLoss Time 1.181s / 10iters, (0.118)\tData load 0.081s / 10iters, (0.008113)\n",
      "Learning rate = [0.009934951447251972, 0.009934951447251972]\tLoss = 2.05236053 (ave = 1.98505024)\n",
      "\n",
      "2023-04-26 12:13:36,697 INFO    [trainer.py, 250] Train Epoch: 0\tTrain Iteration: 300\tTime 19.234s / 10iters, (1.923)\tForward Time 4.941s / 10iters, (0.494)\tBackward Time 13.269s / 10iters, (1.327)\tLoss Time 0.949s / 10iters, (0.095)\tData load 0.075s / 10iters, (0.007454)\n",
      "Learning rate = [0.00993269978678144, 0.00993269978678144]\tLoss = 1.69229960 (ave = 2.14082719)\n",
      "\n",
      "2023-04-26 12:13:56,011 INFO    [trainer.py, 250] Train Epoch: 0\tTrain Iteration: 310\tTime 19.314s / 10iters, (1.931)\tForward Time 4.766s / 10iters, (0.477)\tBackward Time 13.499s / 10iters, (1.350)\tLoss Time 0.966s / 10iters, (0.097)\tData load 0.082s / 10iters, (0.008237)\n",
      "Learning rate = [0.009930448069594734, 0.009930448069594734]\tLoss = 2.28692627 (ave = 2.18255261)\n",
      "\n",
      "2023-04-26 12:14:15,119 INFO    [trainer.py, 250] Train Epoch: 0\tTrain Iteration: 320\tTime 19.108s / 10iters, (1.911)\tForward Time 4.690s / 10iters, (0.469)\tBackward Time 13.343s / 10iters, (1.334)\tLoss Time 0.998s / 10iters, (0.100)\tData load 0.078s / 10iters, (0.007777)\n",
      "Learning rate = [0.009928196295676135, 0.009928196295676135]\tLoss = 2.23669195 (ave = 2.00332296)\n",
      "\n",
      "2023-04-26 12:14:34,728 INFO    [trainer.py, 250] Train Epoch: 0\tTrain Iteration: 330\tTime 19.608s / 10iters, (1.961)\tForward Time 4.751s / 10iters, (0.475)\tBackward Time 13.634s / 10iters, (1.363)\tLoss Time 1.144s / 10iters, (0.114)\tData load 0.079s / 10iters, (0.007924)\n",
      "Learning rate = [0.009925944465009913, 0.009925944465009913]\tLoss = 1.89303589 (ave = 2.34247937)\n",
      "\n",
      "2023-04-26 12:14:54,157 INFO    [trainer.py, 250] Train Epoch: 0\tTrain Iteration: 340\tTime 19.430s / 10iters, (1.943)\tForward Time 4.843s / 10iters, (0.484)\tBackward Time 13.603s / 10iters, (1.360)\tLoss Time 0.902s / 10iters, (0.090)\tData load 0.082s / 10iters, (0.008240)\n",
      "Learning rate = [0.009923692577580339, 0.009923692577580339]\tLoss = 1.88947189 (ave = 2.01919361)\n",
      "\n",
      "2023-04-26 12:15:13,421 INFO    [trainer.py, 250] Train Epoch: 0\tTrain Iteration: 350\tTime 19.263s / 10iters, (1.926)\tForward Time 4.872s / 10iters, (0.487)\tBackward Time 13.218s / 10iters, (1.322)\tLoss Time 1.095s / 10iters, (0.109)\tData load 0.078s / 10iters, (0.007785)\n",
      "Learning rate = [0.009921440633371664, 0.009921440633371664]\tLoss = 2.41348171 (ave = 2.32831618)\n",
      "\n",
      "2023-04-26 12:15:32,524 INFO    [trainer.py, 250] Train Epoch: 0\tTrain Iteration: 360\tTime 19.103s / 10iters, (1.910)\tForward Time 4.800s / 10iters, (0.480)\tBackward Time 13.247s / 10iters, (1.325)\tLoss Time 0.973s / 10iters, (0.097)\tData load 0.083s / 10iters, (0.008325)\n",
      "Learning rate = [0.00991918863236814, 0.00991918863236814]\tLoss = 1.73691535 (ave = 1.98936234)\n",
      "\n",
      "2023-04-26 12:15:51,711 INFO    [trainer.py, 250] Train Epoch: 0\tTrain Iteration: 370\tTime 19.187s / 10iters, (1.919)\tForward Time 4.906s / 10iters, (0.491)\tBackward Time 13.334s / 10iters, (1.333)\tLoss Time 0.866s / 10iters, (0.087)\tData load 0.080s / 10iters, (0.008005)\n",
      "Learning rate = [0.009916936574554001, 0.009916936574554001]\tLoss = 1.94982123 (ave = 2.34575293)\n",
      "\n",
      "2023-04-26 12:16:12,731 INFO    [trainer.py, 250] Train Epoch: 1\tTrain Iteration: 380\tTime 20.952s / 10iters, (2.095)\tForward Time 4.720s / 10iters, (0.472)\tBackward Time 14.425s / 10iters, (1.442)\tLoss Time 1.142s / 10iters, (0.114)\tData load 0.666s / 10iters, (0.066636)\n",
      "Learning rate = [0.009914684459913486, 0.009914684459913486]\tLoss = 1.99807847 (ave = 2.12452935)\n",
      "\n",
      "2023-04-26 12:16:33,039 INFO    [trainer.py, 250] Train Epoch: 1\tTrain Iteration: 390\tTime 20.308s / 10iters, (2.031)\tForward Time 4.772s / 10iters, (0.477)\tBackward Time 14.382s / 10iters, (1.438)\tLoss Time 1.075s / 10iters, (0.108)\tData load 0.079s / 10iters, (0.007923)\n",
      "Learning rate = [0.009912432288430814, 0.009912432288430814]\tLoss = 2.66319370 (ave = 2.17843733)\n",
      "\n",
      "2023-04-26 12:16:53,138 INFO    [trainer.py, 250] Train Epoch: 1\tTrain Iteration: 400\tTime 20.100s / 10iters, (2.010)\tForward Time 4.885s / 10iters, (0.488)\tBackward Time 14.098s / 10iters, (1.410)\tLoss Time 1.032s / 10iters, (0.103)\tData load 0.084s / 10iters, (0.008445)\n",
      "Learning rate = [0.0099101800600902, 0.0099101800600902]\tLoss = 2.21824503 (ave = 1.96774726)\n",
      "\n",
      "2023-04-26 12:17:13,292 INFO    [trainer.py, 250] Train Epoch: 1\tTrain Iteration: 410\tTime 20.154s / 10iters, (2.015)\tForward Time 4.808s / 10iters, (0.481)\tBackward Time 14.158s / 10iters, (1.416)\tLoss Time 1.090s / 10iters, (0.109)\tData load 0.097s / 10iters, (0.009741)\n",
      "Learning rate = [0.009907927774875848, 0.009907927774875848]\tLoss = 1.82445323 (ave = 1.98665651)\n",
      "\n",
      "2023-04-26 12:17:33,547 INFO    [trainer.py, 250] Train Epoch: 1\tTrain Iteration: 420\tTime 20.255s / 10iters, (2.026)\tForward Time 4.835s / 10iters, (0.484)\tBackward Time 14.168s / 10iters, (1.417)\tLoss Time 1.164s / 10iters, (0.116)\tData load 0.088s / 10iters, (0.008811)\n",
      "Learning rate = [0.009905675432771962, 0.009905675432771962]\tLoss = 2.15786791 (ave = 2.04680657)\n",
      "\n",
      "2023-04-26 12:17:53,707 INFO    [trainer.py, 250] Train Epoch: 1\tTrain Iteration: 430\tTime 20.159s / 10iters, (2.016)\tForward Time 4.763s / 10iters, (0.476)\tBackward Time 14.064s / 10iters, (1.406)\tLoss Time 1.260s / 10iters, (0.126)\tData load 0.072s / 10iters, (0.007211)\n",
      "Learning rate = [0.009903423033762725, 0.009903423033762725]\tLoss = 2.26373768 (ave = 2.08016160)\n",
      "\n",
      "2023-04-26 12:18:13,871 INFO    [trainer.py, 250] Train Epoch: 1\tTrain Iteration: 440\tTime 20.164s / 10iters, (2.016)\tForward Time 4.837s / 10iters, (0.484)\tBackward Time 14.241s / 10iters, (1.424)\tLoss Time 1.003s / 10iters, (0.100)\tData load 0.083s / 10iters, (0.008268)\n",
      "Learning rate = [0.009901170577832323, 0.009901170577832323]\tLoss = 1.63451469 (ave = 1.87107145)\n",
      "\n",
      "2023-04-26 12:18:33,673 INFO    [trainer.py, 250] Train Epoch: 1\tTrain Iteration: 450\tTime 19.802s / 10iters, (1.980)\tForward Time 4.833s / 10iters, (0.483)\tBackward Time 13.865s / 10iters, (1.386)\tLoss Time 1.023s / 10iters, (0.102)\tData load 0.081s / 10iters, (0.008071)\n",
      "Learning rate = [0.009898918064964925, 0.009898918064964925]\tLoss = 2.46806240 (ave = 2.13510188)\n",
      "\n",
      "2023-04-26 12:18:53,389 INFO    [trainer.py, 250] Train Epoch: 1\tTrain Iteration: 460\tTime 19.716s / 10iters, (1.972)\tForward Time 4.770s / 10iters, (0.477)\tBackward Time 13.875s / 10iters, (1.388)\tLoss Time 0.993s / 10iters, (0.099)\tData load 0.077s / 10iters, (0.007690)\n",
      "Learning rate = [0.009896665495144698, 0.009896665495144698]\tLoss = 1.77449453 (ave = 1.91230830)\n",
      "\n",
      "2023-04-26 12:19:12,958 INFO    [trainer.py, 250] Train Epoch: 1\tTrain Iteration: 470\tTime 19.569s / 10iters, (1.957)\tForward Time 4.833s / 10iters, (0.483)\tBackward Time 13.544s / 10iters, (1.354)\tLoss Time 1.111s / 10iters, (0.111)\tData load 0.082s / 10iters, (0.008163)\n",
      "Learning rate = [0.009894412868355799, 0.009894412868355799]\tLoss = 1.59980607 (ave = 1.99514037)\n",
      "\n",
      "2023-04-26 12:19:32,538 INFO    [trainer.py, 250] Train Epoch: 1\tTrain Iteration: 480\tTime 19.580s / 10iters, (1.958)\tForward Time 4.767s / 10iters, (0.477)\tBackward Time 13.650s / 10iters, (1.365)\tLoss Time 1.079s / 10iters, (0.108)\tData load 0.084s / 10iters, (0.008391)\n",
      "Learning rate = [0.009892160184582372, 0.009892160184582372]\tLoss = 2.14472294 (ave = 1.95116127)\n",
      "\n",
      "2023-04-26 12:19:52,211 INFO    [trainer.py, 250] Train Epoch: 1\tTrain Iteration: 490\tTime 19.673s / 10iters, (1.967)\tForward Time 4.763s / 10iters, (0.476)\tBackward Time 13.785s / 10iters, (1.379)\tLoss Time 1.042s / 10iters, (0.104)\tData load 0.083s / 10iters, (0.008262)\n",
      "Learning rate = [0.009889907443808557, 0.009889907443808557]\tLoss = 1.94843626 (ave = 2.06837835)\n",
      "\n",
      "2023-04-26 12:20:12,265 INFO    [trainer.py, 250] Train Epoch: 1\tTrain Iteration: 500\tTime 20.053s / 10iters, (2.005)\tForward Time 4.874s / 10iters, (0.487)\tBackward Time 13.966s / 10iters, (1.397)\tLoss Time 1.128s / 10iters, (0.113)\tData load 0.085s / 10iters, (0.008479)\n",
      "Learning rate = [0.009887654646018488, 0.009887654646018488]\tLoss = 2.59677100 (ave = 2.17537717)\n",
      "\n",
      "2023-04-26 12:20:32,142 INFO    [trainer.py, 250] Train Epoch: 1\tTrain Iteration: 510\tTime 19.877s / 10iters, (1.988)\tForward Time 4.737s / 10iters, (0.474)\tBackward Time 14.077s / 10iters, (1.408)\tLoss Time 0.977s / 10iters, (0.098)\tData load 0.086s / 10iters, (0.008572)\n",
      "Learning rate = [0.009885401791196284, 0.009885401791196284]\tLoss = 1.76809072 (ave = 1.98931047)\n",
      "\n",
      "2023-04-26 12:20:51,229 INFO    [trainer.py, 250] Train Epoch: 1\tTrain Iteration: 520\tTime 19.087s / 10iters, (1.909)\tForward Time 4.840s / 10iters, (0.484)\tBackward Time 13.123s / 10iters, (1.312)\tLoss Time 1.041s / 10iters, (0.104)\tData load 0.083s / 10iters, (0.008272)\n",
      "Learning rate = [0.00988314887932606, 0.00988314887932606]\tLoss = 2.30364227 (ave = 2.03431916)\n",
      "\n",
      "2023-04-26 12:21:10,868 INFO    [trainer.py, 250] Train Epoch: 1\tTrain Iteration: 530\tTime 19.639s / 10iters, (1.964)\tForward Time 4.811s / 10iters, (0.481)\tBackward Time 13.668s / 10iters, (1.367)\tLoss Time 1.085s / 10iters, (0.109)\tData load 0.074s / 10iters, (0.007421)\n",
      "Learning rate = [0.009880895910391919, 0.009880895910391919]\tLoss = 1.51798022 (ave = 2.13589725)\n",
      "\n",
      "2023-04-26 12:21:30,218 INFO    [trainer.py, 250] Train Epoch: 1\tTrain Iteration: 540\tTime 19.350s / 10iters, (1.935)\tForward Time 4.791s / 10iters, (0.479)\tBackward Time 13.505s / 10iters, (1.351)\tLoss Time 0.976s / 10iters, (0.098)\tData load 0.078s / 10iters, (0.007761)\n",
      "Learning rate = [0.009878642884377961, 0.009878642884377961]\tLoss = 2.14472532 (ave = 1.88209188)\n",
      "\n",
      "2023-04-26 12:21:49,760 INFO    [trainer.py, 250] Train Epoch: 1\tTrain Iteration: 550\tTime 19.541s / 10iters, (1.954)\tForward Time 4.772s / 10iters, (0.477)\tBackward Time 13.590s / 10iters, (1.359)\tLoss Time 1.096s / 10iters, (0.110)\tData load 0.083s / 10iters, (0.008319)\n",
      "Learning rate = [0.009876389801268273, 0.009876389801268273]\tLoss = 2.06136751 (ave = 2.08792474)\n",
      "\n",
      "2023-04-26 12:22:09,279 INFO    [trainer.py, 250] Train Epoch: 1\tTrain Iteration: 560\tTime 19.519s / 10iters, (1.952)\tForward Time 4.687s / 10iters, (0.469)\tBackward Time 13.608s / 10iters, (1.361)\tLoss Time 1.138s / 10iters, (0.114)\tData load 0.086s / 10iters, (0.008609)\n",
      "Learning rate = [0.009874136661046938, 0.009874136661046938]\tLoss = 2.12969017 (ave = 1.97287524)\n",
      "\n",
      "2023-04-26 12:22:28,735 INFO    [trainer.py, 250] Train Epoch: 1\tTrain Iteration: 570\tTime 19.456s / 10iters, (1.946)\tForward Time 4.681s / 10iters, (0.468)\tBackward Time 13.588s / 10iters, (1.359)\tLoss Time 1.104s / 10iters, (0.110)\tData load 0.083s / 10iters, (0.008335)\n",
      "Learning rate = [0.00987188346369802, 0.00987188346369802]\tLoss = 2.07229924 (ave = 2.13722832)\n",
      "\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!bash scripts/cityscapes/ocrnet/dc_run_r_101_d_8_ocrnet_train.sh train"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "42a4ea106d429adb85b92cbf5fe6fad5ad2431a3e82cdef4435b2cc92f522609"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
